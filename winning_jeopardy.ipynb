{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show Number    Air Date      Round                         Category  Value  \\\n",
       "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
       "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
       "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
       "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
       "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
       "\n",
       "                                            Question      Answer  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "data = pd.read_csv('jeopardy.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing space between column names\n",
    "data.columns = data.columns.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ShowNumber</th>\n",
       "      <th>AirDate</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ShowNumber     AirDate      Round                         Category Value  \\\n",
       "0        4680  2004-12-31  Jeopardy!                          HISTORY  $200   \n",
       "1        4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES  $200   \n",
       "2        4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...  $200   \n",
       "3        4680  2004-12-31  Jeopardy!                 THE COMPANY LINE  $200   \n",
       "4        4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES  $200   \n",
       "\n",
       "                                            Question      Answer  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to normalize the text data in question and answer columns, so we will lowercase them and remove any punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(row):\n",
    "    for punc in string.punctuation:\n",
    "        if punc in row:\n",
    "            row = row.replace(punc, '')\n",
    "    return(row.lower())\n",
    "\n",
    "data['clean_question'] = data['Question'].apply(normalize_string)\n",
    "data['clean_answer'] = data['Answer'].apply(normalize_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_question</th>\n",
       "      <th>clean_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>for the last 8 years of his life galileo was u...</td>\n",
       "      <td>copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no 2 1912 olympian football star at carlisle i...</td>\n",
       "      <td>jim thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the city of yuma in this state has a record av...</td>\n",
       "      <td>arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in 1963 live on the art linkletter show this c...</td>\n",
       "      <td>mcdonalds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>signer of the dec of indep framer of the const...</td>\n",
       "      <td>john adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      clean_question clean_answer\n",
       "0  for the last 8 years of his life galileo was u...   copernicus\n",
       "1  no 2 1912 olympian football star at carlisle i...   jim thorpe\n",
       "2  the city of yuma in this state has a record av...      arizona\n",
       "3  in 1963 live on the art linkletter show this c...    mcdonalds\n",
       "4  signer of the dec of indep framer of the const...   john adams"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['clean_question', 'clean_answer']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert the value column's datatype into numeric and AirDate into datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_value(row):\n",
    "    for punc in string.punctuation:\n",
    "        if punc in row:\n",
    "            row = row.replace(punc, '')\n",
    "    try:\n",
    "        return(int(row))\n",
    "    except ValueError:\n",
    "        return(0)\n",
    "\n",
    "data['clean_value'] = data['Value'].apply(normalize_value)\n",
    "data['AirDate'] = pd.to_datetime(data['AirDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_value</th>\n",
       "      <th>AirDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>2004-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>2004-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>2004-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>2004-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>2004-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clean_value    AirDate\n",
       "0          200 2004-12-31\n",
       "1          200 2004-12-31\n",
       "2          200 2004-12-31\n",
       "3          200 2004-12-31\n",
       "4          200 2004-12-31"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['clean_value', 'AirDate']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main ways of studying for Jeopardy would be to study questions from previous episodes, study general knowledge or just winging it! So we want to look at two things here:\n",
    "\n",
    "1) How often the answer is deducible from the question - we will see how many times words in the answer also occur in the question.\n",
    "\n",
    "2) How often new questions are repeats of older questions - we will assess how often 'complex' words (more than 6 characters) reoccur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean percentage of words that appear in both the answers and questions is 5.89%\n"
     ]
    }
   ],
   "source": [
    "#writing a function to match words in question and answer\n",
    "def match_answer(row):\n",
    "    split_answer = row['clean_answer'].split()\n",
    "    split_question = row['clean_question'].split()\n",
    "    match_count = 0\n",
    "    if 'the' in split_answer:\n",
    "        split_answer.remove('the')\n",
    "    if len(split_answer) == 0:\n",
    "        return 0\n",
    "    for word in split_answer:\n",
    "        if word in split_question:\n",
    "            match_count += 1\n",
    "    return(match_count / len(split_answer))\n",
    "\n",
    "data['answer_in_question'] = data.apply(match_answer, axis=1)\n",
    "print('The mean percentage of words that appear in both the answers and questions is ' + \n",
    "      str(round(data['answer_in_question'].mean() * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.89% of words appear in both questions and answers - that is not a lot! It might not be the best stretagey to attempt deducing answers from questions. Well then, should we bother studying questions from previous episodes? To find out, we'll keep track of all the complex words (more than 5 words) and see if questions contain any of these words. We will get a percentage of complex words within questions that match with our word bank and come up with a percentage of the matches per question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6919577992203563"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_overlap = []\n",
    "terms_used = {} #master list of 'complex' (more than 5 characters) words used\n",
    "for index, row in data.iterrows():\n",
    "    split_question = row['clean_question'].split(' ') #splitting question into a list of words\n",
    "    split_question = [word for word in split_question if len(word) > 5] #only keeping words with 6 or more characters\n",
    "    match_count = 0\n",
    "    for word in split_question:\n",
    "        if word in terms_used:\n",
    "            match_count += 1 #if word in question matches with any of the words used in the past, add point\n",
    "            terms_used[word] += 1\n",
    "        else:\n",
    "            terms_used[word] = 0\n",
    "    if len(split_question) > 0:\n",
    "        match_count /= len(split_question) #getting the overall percentage of words matched with words from previous questions\n",
    "    question_overlap.append(match_count)\n",
    "\n",
    "data['question_overlap'] = question_overlap\n",
    "data['question_overlap'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% of mean percentage of match may seem high, but we need to note that we are only looking at words with 6 or more characters, not the questions themselves. Nonetheless, let's stick with this analysis and find the most efficient way of answering high valued questions. That is, which of the words within our word bank are in questions with high value (more than 800 USD prize money)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_value(row):\n",
    "    if row['clean_value'] > 800:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data['high_value'] = data.apply(assign_value, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take a look at the 20 most commonly found words from our dataset and run chi-square tests. These are the main steps we will be taking:\n",
    "- Find the number of low value questions the word occurs in.\n",
    "- Find the number of high value questions the word occurs in.\n",
    "- Find the percentage of questions the word occurs in.\n",
    "- Based on the percentage of questions the word occurs in, find expected counts.\n",
    "- Compute the chi squared value based on the expected counts and the observed counts for high and low value questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_terms_used = sorted(terms_used.items(), key=operator.itemgetter(1), reverse=True)\n",
    "example_terms_used = []\n",
    "for pair in sorted_terms_used[:50]:\n",
    "    example_terms_used.append(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(168, 346),\n",
       " (141, 332),\n",
       " (77, 212),\n",
       " (79, 203),\n",
       " (71, 191),\n",
       " (68, 181),\n",
       " (61, 186),\n",
       " (77, 173),\n",
       " (78, 168),\n",
       " (97, 146),\n",
       " (108, 133),\n",
       " (73, 134),\n",
       " (57, 124),\n",
       " (55, 124),\n",
       " (42, 134),\n",
       " (55, 108),\n",
       " (50, 119),\n",
       " (54, 110),\n",
       " (52, 108),\n",
       " (58, 102),\n",
       " (45, 115),\n",
       " (54, 102),\n",
       " (46, 107),\n",
       " (42, 110),\n",
       " (43, 108),\n",
       " (34, 112),\n",
       " (51, 92),\n",
       " (36, 103),\n",
       " (52, 89),\n",
       " (40, 100),\n",
       " (62, 79),\n",
       " (65, 73),\n",
       " (43, 95),\n",
       " (41, 79),\n",
       " (30, 85),\n",
       " (36, 79),\n",
       " (22, 97),\n",
       " (26, 84),\n",
       " (26, 84),\n",
       " (30, 74),\n",
       " (31, 72),\n",
       " (45, 54),\n",
       " (44, 54),\n",
       " (22, 76),\n",
       " (23, 74),\n",
       " (35, 62),\n",
       " (25, 68),\n",
       " (34, 61),\n",
       " (25, 69),\n",
       " (28, 67)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a function that takes in a word and counts how many times these words are in high and low valued questions\n",
    "def value_count(word):\n",
    "    low_count = 0\n",
    "    high_count = 0\n",
    "    for index, row in data.iterrows():\n",
    "        if word in row['clean_question'].split(' '):\n",
    "            if row['high_value'] == 1:\n",
    "                high_count += 1\n",
    "            else:\n",
    "                low_count += 1\n",
    "    return(high_count, low_count)\n",
    "\n",
    "observed_expected = []\n",
    "\n",
    "for word in example_terms_used:\n",
    "    observed_expected.append(value_count(word))\n",
    "\n",
    "observed_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Power_divergenceResult(statistic=4.048305063534577, pvalue=0.044215717944225866),\n",
       " Power_divergenceResult(statistic=0.29967829483482744, pvalue=0.5840841713114313),\n",
       " Power_divergenceResult(statistic=0.5810990283039111, pvalue=0.4458818590919339),\n",
       " Power_divergenceResult(statistic=0.05956570730840162, pvalue=0.8071836789959332),\n",
       " Power_divergenceResult(statistic=0.3166666119780599, pvalue=0.5736178092344641),\n",
       " Power_divergenceResult(statistic=0.22592591114717697, pvalue=0.6345612982626103),\n",
       " Power_divergenceResult(statistic=1.9084254764809114, pvalue=0.16713826420470967),\n",
       " Power_divergenceResult(statistic=0.55386193833867, pvalue=0.45674398774097136),\n",
       " Power_divergenceResult(statistic=1.108644756518943, pvalue=0.2923767382010634),\n",
       " Power_divergenceResult(statistic=15.028296538003147, pvalue=0.00010591119029347305),\n",
       " Power_divergenceResult(statistic=30.70509560211122, pvalue=3.003751982853918e-08),\n",
       " Power_divergenceResult(statistic=4.401396413478652, pvalue=0.03590951591318824),\n",
       " Power_divergenceResult(statistic=0.7039630431807515, pvalue=0.40145524983702063),\n",
       " Power_divergenceResult(statistic=0.36956355622281933, pvalue=0.5432422635312689),\n",
       " Power_divergenceResult(statistic=1.9892622715198827, pvalue=0.1584180367255449),\n",
       " Power_divergenceResult(statistic=2.049483384913445, pvalue=0.15225784710260337),\n",
       " Power_divergenceResult(statistic=0.06908968634314422, pvalue=0.7926668351740855),\n",
       " Power_divergenceResult(statistic=1.4521478773041714, pvalue=0.22818361990917996),\n",
       " Power_divergenceResult(statistic=1.1467782632567483, pvalue=0.2842245888520029),\n",
       " Power_divergenceResult(statistic=4.4934633334396965, pvalue=0.03402468062121473),\n",
       " Power_divergenceResult(statistic=0.023360483433014848, pvalue=0.8785233749036424),\n",
       " Power_divergenceResult(statistic=2.6950209285044178, pvalue=0.10066216730100558),\n",
       " Power_divergenceResult(statistic=0.1453643483185807, pvalue=0.7030053100373672),\n",
       " Power_divergenceResult(statistic=0.08036666360833383, pvalue=0.7768011333475542),\n",
       " Power_divergenceResult(statistic=0.0027964365481966506, pvalue=0.9578264488892704),\n",
       " Power_divergenceResult(statistic=2.069244036259131, pvalue=0.1502960035231485),\n",
       " Power_divergenceResult(statistic=3.4193064786286467, pvalue=0.06443807841398486),\n",
       " Power_divergenceResult(statistic=0.5223195571633583, pvalue=0.4698538986923181),\n",
       " Power_divergenceResult(statistic=4.644947440199665, pvalue=0.031145090879057265),\n",
       " Power_divergenceResult(statistic=0.0006846341565692503, pvalue=0.9791253217871299),\n",
       " Power_divergenceResult(statistic=16.1398795828231, pvalue=5.883208699870318e-05),\n",
       " Power_divergenceResult(statistic=22.92015726204268, pvalue=1.6887164902326845e-06),\n",
       " Power_divergenceResult(statistic=0.41769790572413584, pvalue=0.5180879853877616),\n",
       " Power_divergenceResult(statistic=1.771905597365104, pvalue=0.18314642208773116),\n",
       " Power_divergenceResult(statistic=0.3756040760247317, pvalue=0.5399653011508676),\n",
       " Power_divergenceResult(statistic=0.3898148396824147, pvalue=0.5323967459066039),\n",
       " Power_divergenceResult(statistic=6.034956335550907, pvalue=0.014025297767102603),\n",
       " Power_divergenceResult(statistic=1.3636119408688154, pvalue=0.24291248030119686),\n",
       " Power_divergenceResult(statistic=1.3636119408688154, pvalue=0.24291248030119686),\n",
       " Power_divergenceResult(statistic=0.0015524121819626294, pvalue=0.9685709519600363),\n",
       " Power_divergenceResult(statistic=0.10236523249330269, pvalue=0.7490095154898175),\n",
       " Power_divergenceResult(statistic=13.635380774012475, pvalue=0.00022196284405439382),\n",
       " Power_divergenceResult(statistic=12.617236716294089, pvalue=0.000382205920241825),\n",
       " Power_divergenceResult(statistic=1.8553957546080455, pvalue=0.17315713805561117),\n",
       " Power_divergenceResult(statistic=1.1669131483229773, pvalue=0.2800364143886497),\n",
       " Power_divergenceResult(statistic=2.605056949241688, pvalue=0.1065233308712032),\n",
       " Power_divergenceResult(statistic=0.1456588573979392, pvalue=0.7027189169311097),\n",
       " Power_divergenceResult(statistic=2.353594083329848, pvalue=0.1249945297234204),\n",
       " Power_divergenceResult(statistic=0.19803386869565398, pvalue=0.6563125529496141),\n",
       " Power_divergenceResult(statistic=0.029897222578856228, pvalue=0.8627236384611155)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_value_count = data[data['high_value'] == 1].shape[0]\n",
    "low_value_count = data[data['high_value'] == 0].shape[0]\n",
    "\n",
    "chi_squared = []\n",
    "for values in observed_expected:\n",
    "    total = sum(values)\n",
    "    total_prop = total / data.shape[0] #the percentage of questions the word occurs in\n",
    "    high_prop = total_prop * high_value_count #finding expected counts based on percentage within sample\n",
    "    low_prop = total_prop * low_value_count\n",
    "    \n",
    "    observed = np.array([values[0], values[1]])\n",
    "    expected = np.array([high_prop, low_prop])\n",
    "    chisq = chisquare(observed, expected)\n",
    "    chi_squared.append(chisq)\n",
    "\n",
    "chi_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "called\n",
      "targetblankherea\n",
      "french\n",
      "island\n",
      "meaning\n",
      "founded\n",
      "reports\n",
      "targetblankthisa\n",
      "popular\n",
      "italian\n",
      "german\n"
     ]
    }
   ],
   "source": [
    "#printing out words with significant chi-square scores\n",
    "for i, stats in enumerate(chi_squared):\n",
    "    if stats[1] < 0.05: #p=0.05\n",
    "        print(example_terms_used[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the main findings that might help contestants to narrow down which topics they should study if they are short on time and want to target highly valued questions:\n",
    "- French, Italian and German were significant. Focus on European countries!\n",
    "- Study a bit on islands as well.\n",
    "- Take a look at some major companies that people 'founded' throughout history.\n",
    "\n",
    "Other words are either too vague or too common. If we want a more robust analysis, we may want increase our sample size in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
